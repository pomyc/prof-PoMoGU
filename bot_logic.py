import os
import re
from flask import jsonify
from seniority_calculator import calculate_seniority
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
from openai import OpenAI
from collections import Counter

# API –∫–ª—é—á OpenAI
openai_api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=openai_api_key)

if not openai_api_key:
    print("‚ùå OpenAI API key not found!")
else:
    print(f"‚úÖ OpenAI API key loaded: {openai_api_key[:10]}...")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
try:
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.load_local(
        "./knowledge_base", 
        embeddings,
        allow_dangerous_deserialization=True
    )
    print("üì¶ –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ FAISS —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏: {e}")
    vectorstore = None

# –°—Ç–∞–Ω –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
user_state = {}

# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
def handle_message(data):
    message_obj = data.get("message", {})
    message = message_obj.get("text")
    chat_id = message_obj.get("chat", {}).get("id")
    user_id = message_obj.get("from", {}).get("id")

    if not message or not chat_id:
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id or 0,
            "text": "‚ö†Ô∏è –Ø —Ä–æ–∑—É–º—ñ—é —Ç—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."
        })

    if message.strip().lower() in ["/start", "start"]:
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üëã –í—ñ—Ç–∞—é! –Ø –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–æ–≤–∏–π –ø–æ–º—ñ—á–Ω–∏–∫. –û–±–µ—Ä—ñ—Ç—å –¥—ñ—é:",
            "reply_markup": {
                "keyboard": [
                    [{"text": "üìã –ó–∞–¥–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è"}],
                    [{"text": "üìö –ó–∞–ø–∏—Ç –¥–æ –±–∞–∑–∏ –∑–Ω–∞–Ω—å"}],
                    [{"text": "üìÖ –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ç—Ä—É–¥–æ–≤–æ–≥–æ —Å—Ç–∞–∂—É"}],
                    [{"text": "üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏"}]
                ],
                "resize_keyboard": True
            }
        })

    if message == "üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏":
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üìç –î–Ω—ñ–ø—Ä–æ, –ø—Ä. –î.–Ø–≤–æ—Ä–Ω–∏—Ü—å–∫–æ–≥–æ, 93, –æ—Ñ. 327\nüìû 050 324-54-11\nüìß profpmgu@gmail.com\nüåê http://pmguinfo.dp.ua"
        })

    if message == "üìÖ –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ç—Ä—É–¥–æ–≤–æ–≥–æ —Å—Ç–∞–∂—É":
        user_state[user_id] = "awaiting_seniority_input"
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üìÖ –í–≤–µ–¥—ñ—Ç—å –¥–∞—Ç—É –ø–æ—á–∞—Ç–∫—É —Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ —á–µ—Ä–µ–∑ –∫—Ä–∞–ø–∫—É –∑ –∫–æ–º–æ—é (;)\n–ù–∞–ø—Ä–∏–∫–ª–∞–¥:\n01.09.2015; 24.04.2025\n–∞–±–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É –¥–∞—Ç—É, —è–∫—â–æ –¥–æ—Å—ñ –ø—Ä–∞—Ü—é—î—Ç–µ."
        })

    if user_state.get(user_id) == "awaiting_seniority_input":
        reply = calculate_seniority_input(message)
        user_state.pop(user_id, None)
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": reply
        })

    if message == "üìö –ó–∞–ø–∏—Ç –¥–æ –±–∞–∑–∏ –∑–Ω–∞–Ω—å":
        user_state[user_id] = "awaiting_knowledge_query"
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üìö –í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç, —ñ —è —Å–ø—Ä–æ–±—É—é –∑–Ω–∞–π—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å:"
        })

    if user_state.get(user_id) == "awaiting_knowledge_query":
        reply = search_in_knowledge_base(message)
        user_state.pop(user_id, None)
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": reply
        })

    # GPT ‚Äî –æ—Å–Ω–æ–≤–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
    reply = ask_gpt_with_smart_context(message)
    return jsonify({
        "method": "sendMessage",
        "chat_id": chat_id,
        "text": reply
    })

# –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞–∂—É
def calculate_seniority_input(message):
    try:
        if ";" in message:
            start_date, end_date = [d.strip() for d in message.split(";")]
            return calculate_seniority(start_date, end_date)
        else:
            return calculate_seniority(message.strip())
    except Exception:
        return "‚ö†Ô∏è –ù–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç. –ù–∞–ø–∏—à—ñ—Ç—å, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥:\n01.09.2015; 24.04.2025"

# GPT-–≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å
def preprocess_query(query):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    query = query.lower()
    
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    query = re.sub(r'[^\w\s]', ' ', query)
    query = re.sub(r'\s+', ' ', query).strip()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keywords = query.split()
    
    # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    synonyms = {
        '–≤–Ω–µ—Å–∫–∏': ['–≤–Ω–µ—Å–æ–∫', '–ø–ª–∞—Ç–∞', '–≤–∑–Ω–æ—Å', '–æ–ø–ª–∞—Ç–∞', '–∫–æ—à—Ç–∏'],
        '—Ä–æ–∑–º—ñ—Ä': ['—Å—É–º–∞', '–≤–µ–ª–∏—á–∏–Ω–∞', '—Ä–æ–∑–º—ñ—Ä', '–∫—ñ–ª—å–∫—ñ—Å—Ç—å'],
        '–ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–æ–≤—ñ': ['–ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∞', '–ø—Ä–æ—Ñ–∫–æ–º', '–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è'],
        '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–æ–±—ñ—Ç–Ω–∞ –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ –ø—Ä–∞—Ü—ñ', '–≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞'],
        '–≤—ñ–¥–ø—É—Å—Ç–∫–∞': ['–≤—ñ–¥–ø–æ—á–∏–Ω–æ–∫', '–∫–∞–Ω—ñ–∫—É–ª–∏', '–≤–∏—Ö—ñ–¥–Ω—ñ'],
        '–∑–≤—ñ–ª—å–Ω–µ–Ω–Ω—è': ['—Ä–æ–∑—ñ—Ä–≤–∞–Ω–Ω—è', '–ø—Ä–∏–ø–∏–Ω–µ–Ω–Ω—è', '–∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è']
    }
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    expanded_keywords = keywords.copy()
    for keyword in keywords:
        for base_word, synonym_list in synonyms.items():
            if keyword in synonym_list or keyword == base_word:
                expanded_keywords.extend(synonym_list)
    
    return ' '.join(list(set(expanded_keywords)))

def calculate_relevance_score(result, query_keywords):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    content = result.page_content.lower()
    query_words = query_keywords.lower().split()
    
    score = 0
    content_words = re.findall(r'\w+', content)
    content_counter = Counter(content_words)
    
    for word in query_words:
        if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if word in content_words:
                score += 10
            
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            for content_word in content_words:
                if word in content_word or content_word in word:
                    score += 5
    
    # –ë–æ–Ω—É—Å –∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
    source = result.metadata.get('source', '').lower()
    if any(keyword in source for keyword in ['–≤–Ω–µ—Å–æ–∫', '–ø–ª–∞—Ç—ñ–∂', '—Å—É–º–∞', '—Ä–æ–∑–º—ñ—Ä']):
        score += 15
    
    return score

def search_in_knowledge_base(query):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    try:
        if not vectorstore:
            return "‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–ø—Ä–æ–±—É–π—Ç–µ –ø—ñ–∑–Ω—ñ—à–µ."
        
        print(f"üîç –ü–æ—à—É–∫ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å: {query}")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        processed_query = preprocess_query(query)
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {processed_query}")
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        results = vectorstore.similarity_search(processed_query, k=10)
        
        if not results:
            # –ü—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
            results = vectorstore.similarity_search(query, k=10)
        
        if not results:
            return "üìö –£ –±–∞–∑—ñ –∑–Ω–∞–Ω—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ —Ü–µ –ø–∏—Ç–∞–Ω–Ω—è."
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        scored_results = []
        for result in results:
            score = calculate_relevance_score(result, query + ' ' + processed_query)
            scored_results.append((result, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_results.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ (—Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º)
        relevant_results = [result for result, score in scored_results if score > 5][:3]
        
        if not relevant_results:
            return "üìö –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –±–∞–∑—ñ –∑–Ω–∞–Ω—å. –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑—É–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = "üìñ –ù–∞–π–∫—Ä–∞—â–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å:\n\n"
        
        for i, result in enumerate(relevant_results, 1):
            source = result.metadata.get('source', '–ù–µ–≤—ñ–¥–æ–º–µ –¥–∂–µ—Ä–µ–ª–æ')
            content = result.page_content.strip()
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if len(content) > 500:
                content = content[:500] + "..."
            
            response += f"üìÑ –î–∂–µ—Ä–µ–ª–æ: {source}\n"
            response += f"{content}\n"
            
            if i < len(relevant_results):
                response += "\n" + "="*30 + "\n\n"
        
        return response
        
    except Exception as e:
        print(f"‚ùå DB error: {e}")
        return "‚ö†Ô∏è –°—Ç–∞–ª–∞—Å—è –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ—à—É–∫—É –≤ –±–∞–∑—ñ –∑–Ω–∞–Ω—å."

def ask_gpt_with_smart_context(message):
    """GPT —Å —É–º–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    try:
        context = ""
        
        if vectorstore:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                processed_query = preprocess_query(message)
                results = vectorstore.similarity_search(processed_query, k=5)
                
                if results:
                    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    scored_results = []
                    for result in results:
                        score = calculate_relevance_score(result, message + ' ' + processed_query)
                        scored_results.append((result, score))
                    
                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    relevant_results = [result for result, score in scored_results if score > 10][:2]
                    
                    if relevant_results:
                        context = "\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å:\n"
                        for result in relevant_results:
                            source = result.metadata.get('source', '–¥–æ–∫—É–º–µ–Ω—Ç')
                            content = result.page_content[:400]
                            context += f"–ó {source}: {content}...\n\n"
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {e}")

        system_message = (
            "–¢–∏ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–æ–≤–∏–π –ø–æ–º—ñ—á–Ω–∏–∫-–µ–∫—Å–ø–µ—Ä—Ç –∑ —Ç—Ä—É–¥–æ–≤–æ–≥–æ –ø—Ä–∞–≤–∞ –£–∫—Ä–∞—ó–Ω–∏. "
            "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –Ω–∞–¥–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
            "–Ø–∫—â–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ —î —Ç–æ—á–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —ó—ó. "
            "–Ø–∫—â–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π - –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–∞–≥–∞–ª—å–Ω–∏—Ö –∑–Ω–∞–Ω—å –ø—Ä–æ —Ç—Ä—É–¥–æ–≤–µ –ø—Ä–∞–≤–æ. "
            "–ü–æ—Å–∏–ª–∞–π—Å—è –Ω–∞ —Å—Ç–∞—Ç—Ç—ñ –ö–ó–ø–ü –£–∫—Ä–∞—ó–Ω–∏, —è–∫—â–æ —Ü–µ –¥–æ—Ä–µ—á–Ω–æ. "
            "–°—Ç—Ä—É–∫—Ç—É—Ä—É–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å —á—ñ—Ç–∫–æ —Ç–∞ –ª–æ–≥—ñ—á–Ω–æ."
        )

        user_message = f"–ü–∏—Ç–∞–Ω–Ω—è: {message}"
        if context:
            user_message += f"\n{context}"

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"‚ùå GPT error: {e}")
        return (
            "üîç –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å.\n"
            "üìç –ó–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏:\n"
            "–î–Ω—ñ–ø—Ä–æ, –ø—Ä. –î.–Ø–≤–æ—Ä–Ω–∏—Ü—å–∫–æ–≥–æ, 93, –∫.327\n"
            "üìû 050 324-54-11\n"
            "üìß profpmgu@gmail.com"
        )
