import os
import re
from flask import jsonify
from seniority_calculator import calculate_seniority
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
from openai import OpenAI
from collections import Counter

# API –∫–ª—é—á OpenAI
openai_api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=openai_api_key)

if not openai_api_key:
    print("‚ùå OpenAI API key not found!")
else:
    print(f"‚úÖ OpenAI API key loaded: {openai_api_key[:10]}...")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
try:
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.load_local(
        "./knowledge_base", 
        embeddings,
        allow_dangerous_deserialization=True
    )
    print("üì¶ –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ FAISS —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏: {e}")
    vectorstore = None

# –°—Ç–∞–Ω –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
user_state = {}

# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
def handle_message(data):
    message_obj = data.get("message", {})
    message = message_obj.get("text")
    chat_id = message_obj.get("chat", {}).get("id")
    user_id = message_obj.get("from", {}).get("id")

    if not message or not chat_id:
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id or 0,
            "text": "‚ö†Ô∏è –Ø —Ä–æ–∑—É–º—ñ—é —Ç—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."
        })

    if message.strip().lower() in ["/start", "start"]:
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üëã –í—ñ—Ç–∞—é! –Ø –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–æ–≤–∏–π –ø–æ–º—ñ—á–Ω–∏–∫. –û–±–µ—Ä—ñ—Ç—å –¥—ñ—é:",
            "reply_markup": {
                "keyboard": [
                    [{"text": "üìã –ó–∞–¥–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è"}],
                    [{"text": "üìö –ó–∞–ø–∏—Ç –¥–æ –±–∞–∑–∏ –∑–Ω–∞–Ω—å"}],
                    [{"text": "üìÖ –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ç—Ä—É–¥–æ–≤–æ–≥–æ —Å—Ç–∞–∂—É"}],
                    [{"text": "üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏"}]
                ],
                "resize_keyboard": True
            }
        })

    if message == "üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏":
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üìç –î–Ω—ñ–ø—Ä–æ, –ø—Ä. –î.–Ø–≤–æ—Ä–Ω–∏—Ü—å–∫–æ–≥–æ, 93, –æ—Ñ. 327\nüìû 050 324-54-11\nüìß profpmgu@gmail.com\nüåê http://pmguinfo.dp.ua"
        })

    if message == "üìÖ –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ç—Ä—É–¥–æ–≤–æ–≥–æ —Å—Ç–∞–∂—É":
        user_state[user_id] = "awaiting_seniority_input"
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üìÖ –í–≤–µ–¥—ñ—Ç—å –¥–∞—Ç—É –ø–æ—á–∞—Ç–∫—É —Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ —á–µ—Ä–µ–∑ –∫—Ä–∞–ø–∫—É –∑ –∫–æ–º–æ—é (;)\n–ù–∞–ø—Ä–∏–∫–ª–∞–¥:\n01.09.2015; 24.04.2025\n–∞–±–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É –¥–∞—Ç—É, —è–∫—â–æ –¥–æ—Å—ñ –ø—Ä–∞—Ü—é—î—Ç–µ."
        })

    if user_state.get(user_id) == "awaiting_seniority_input":
        reply = calculate_seniority_input(message)
        user_state.pop(user_id, None)
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": reply
        })

    if message == "üìö –ó–∞–ø–∏—Ç –¥–æ –±–∞–∑–∏ –∑–Ω–∞–Ω—å":
        user_state[user_id] = "awaiting_knowledge_query"
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": "üìö –í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç, —ñ —è —Å–ø—Ä–æ–±—É—é –∑–Ω–∞–π—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å:"
        })

    if user_state.get(user_id) == "awaiting_knowledge_query":
        reply = search_in_knowledge_base(message)
        user_state.pop(user_id, None)
        return jsonify({
            "method": "sendMessage",
            "chat_id": chat_id,
            "text": reply
        })

    # GPT ‚Äî –æ—Å–Ω–æ–≤–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
    reply = ask_gpt_with_smart_context(message)
    return jsonify({
        "method": "sendMessage",
        "chat_id": chat_id,
        "text": reply
    })

# –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞–∂—É
def calculate_seniority_input(message):
    try:
        if ";" in message:
            start_date, end_date = [d.strip() for d in message.split(";")]
            return calculate_seniority(start_date, end_date)
        else:
            return calculate_seniority(message.strip())
    except Exception:
        return "‚ö†Ô∏è –ù–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç. –ù–∞–ø–∏—à—ñ—Ç—å, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥:\n01.09.2015; 24.04.2025"

def extract_key_concepts(query):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
    query_lower = query.lower()
    
    concepts = {
        '–≤–∑–Ω–æ—Å—ã': ['–≤–Ω–µ—Å–æ–∫', '–≤–Ω–µ—Å–∫–∏', '–≤–∑–Ω–æ—Å', '–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç—ñ–∂', '–æ–ø–ª–∞—Ç–∞', '–∫–æ—à—Ç–∏', '—Å—É–º–∞', '—Ä–æ–∑–º—ñ—Ä'],
        '–≤—ã–±–æ—Ä—ã': ['–≤–∏–±–æ—Ä–∏', '–≤–∏–±–æ—Ä—á–∞', '–∑–≤—ñ—Ç–Ω–æ-–≤–∏–±–æ—Ä—á–∞', '–∫–∞–º–ø–∞–Ω—ñ—è', '–≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è'],
        '–æ—Ç–ø—É—Å–∫': ['–≤—ñ–¥–ø—É—Å—Ç–∫–∞', '–≤—ñ–¥–ø–æ—á–∏–Ω–æ–∫', '–∫–∞–Ω—ñ–∫—É–ª–∏'],
        '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': ['–∑–≤—ñ–ª—å–Ω–µ–Ω–Ω—è', '—Ä–æ–∑—ñ—Ä–≤–∞–Ω–Ω—è', '–ø—Ä–∏–ø–∏–Ω–µ–Ω–Ω—è'],
        '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–æ–±—ñ—Ç–Ω–∞ –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ –ø—Ä–∞—Ü—ñ', '–≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞'],
        '—Ä–∞–±–æ—Ç–∞': ['—Ä–æ–±–æ—Ç–∞', '–ø—Ä–∞—Ü—è', '—Ç—Ä—É–¥–æ–≤–∏–π', '—Å–ª—É–∂–±–æ–≤–∏–π'],
        '–ø—Ä–æ—Ñ—Å–æ—é–∑': ['–ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∞', '–ø—Ä–æ—Ñ–∫–æ–º', '–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è']
    }
    
    found_concepts = []
    for concept, keywords in concepts.items():
        if any(keyword in query_lower for keyword in keywords):
            found_concepts.append(concept)
    
    return found_concepts

def calculate_semantic_relevance(result, query, query_concepts):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    content = result.page_content.lower()
    source = result.metadata.get('source', '').lower()
    
    score = 0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
    content_concepts = extract_key_concepts(content)
    
    # –í—ã—Å–æ–∫–∏–π –±–∞–ª–ª –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
    common_concepts = set(query_concepts) & set(content_concepts)
    score += len(common_concepts) * 50
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –≤–∑–Ω–æ—Å–∞—Ö
    if '–≤–∑–Ω–æ—Å—ã' in query_concepts:
        money_keywords = ['–≥—Ä–Ω', '–≥—Ä–∏–≤–µ–Ω—å', '–∫–æ–ø—ñ–π–æ–∫', '–ø—Ä–æ—Ü–µ–Ω—Ç', '%', '—Ä–æ–∑–º—ñ—Ä', '—Å—É–º–∞', '—Å—Ç–∞–≤–∫–∞', '—Ç–∞—Ä–∏—Ñ']
        if any(keyword in content for keyword in money_keywords):
            score += 100
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ –≤—ã–±–æ—Ä–∞—Ö –ø—Ä–∏ –≤–æ–ø—Ä–æ—Å–µ –æ –≤–∑–Ω–æ—Å–∞—Ö
        if '–≤—ã–±–æ—Ä—ã' in content_concepts and '–≤–∑–Ω–æ—Å—ã' not in content_concepts:
            score -= 200
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if '–≤–∑–Ω–æ—Å—ã' in query_concepts:
        if any(word in source for word in ['–≤–Ω–µ—Å–æ–∫', '–ø–ª–∞—Ç–∞', '—Ñ—ñ–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç']):
            score += 30
        if any(word in source for word in ['–≤–∏–±–æ—Ä', '–∑–≤—ñ—Ç']):
            score -= 50
    
    # –¢–æ—á–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (—Å –º–µ–Ω—å—à–∏–º –≤–µ—Å–æ–º)
    query_words = set(re.findall(r'\w+', query.lower()))
    content_words = set(re.findall(r'\w+', content))
    
    exact_matches = query_words & content_words
    score += len(exact_matches) * 5
    
    return score

def search_in_knowledge_base(query):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    try:
        if not vectorstore:
            return "‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–ø—Ä–æ–±—É–π—Ç–µ –ø—ñ–∑–Ω—ñ—à–µ."
        
        print(f"üîç –ü–æ—à—É–∫ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å: {query}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_concepts = extract_key_concepts(query)
        print(f"üéØ –í–∏—è–≤–ª–µ–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó: {query_concepts}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = vectorstore.similarity_search(query, k=15)
        
        if not results:
            return "üìö –£ –±–∞–∑—ñ –∑–Ω–∞–Ω—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ —Ü–µ –ø–∏—Ç–∞–Ω–Ω—è."
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        scored_results = []
        for result in results:
            score = calculate_semantic_relevance(result, query, query_concepts)
            scored_results.append((result, score))
            print(f"üìÑ {result.metadata.get('source', 'unknown')}: score={score}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_results.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        min_score = 50 if query_concepts else 20
        relevant_results = [result for result, score in scored_results if score >= min_score][:3]
        
        if not relevant_results:
            return "üìö –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–ª—è –≤–∞—à–æ–≥–æ –∑–∞–ø–∏—Ç—É. –ú–æ–∂–ª–∏–≤–æ, –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑—É–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –∑–≤–µ—Ä–Ω—É—Ç–∏—Å—è –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –¥–æ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = "üìñ –ó–Ω–∞–π–¥–µ–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å:\n\n"
        
        for i, result in enumerate(relevant_results, 1):
            source = result.metadata.get('source', '–ù–µ–≤—ñ–¥–æ–º–µ –¥–∂–µ—Ä–µ–ª–æ')
            content = result.page_content.strip()
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if len(content) > 400:
                content = content[:400] + "..."
            
            response += f"üìÑ –î–∂–µ—Ä–µ–ª–æ: {source}\n"
            response += f"{content}\n"
            
            if i < len(relevant_results):
                response += "\n" + "="*30 + "\n\n"
        
        return response
        
    except Exception as e:
        print(f"‚ùå DB error: {e}")
        return "‚ö†Ô∏è –°—Ç–∞–ª–∞—Å—è –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ—à—É–∫—É –≤ –±–∞–∑—ñ –∑–Ω–∞–Ω—å."

def ask_gpt_with_smart_context(message):
    """GPT —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    try:
        context = ""
        
        if vectorstore:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                query_concepts = extract_key_concepts(message)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
                results = vectorstore.similarity_search(message, k=10)
                
                if results:
                    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    scored_results = []
                    for result in results:
                        score = calculate_semantic_relevance(result, message, query_concepts)
                        scored_results.append((result, score))
                    
                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    min_score = 30 if query_concepts else 15
                    relevant_results = [result for result, score in scored_results if score >= min_score][:2]
                    
                    if relevant_results:
                        context = "\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å:\n"
                        for result in relevant_results:
                            source = result.metadata.get('source', '–¥–æ–∫—É–º–µ–Ω—Ç')
                            content = result.page_content[:400]
                            context += f"–ó {source}: {content}...\n\n"
                    else:
                        print("‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É")
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {e}")

        system_message = (
            "–¢–∏ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π –ø–æ–º—ñ—á–Ω–∏–∫ –∑ –ø–∏—Ç–∞–Ω—å –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–æ–≤–æ—ó –¥—ñ—è–ª—å–Ω–æ—Å—Ç—ñ —Ç–∞ —Ç—Ä—É–¥–æ–≤–æ–≥–æ –ø—Ä–∞–≤–∞ –£–∫—Ä–∞—ó–Ω–∏. "
            "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é, –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ —Ç–∞ —Ç–æ—á–Ω–æ. "
            "–Ø–∫—â–æ –≤ –Ω–∞–¥–∞–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ —î —Ç–æ—á–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —ó—ó. "
            "–Ø–∫—â–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∞–±–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π - —á–µ—Å–Ω–æ —Å–∫–∞–∂–∏ –ø—Ä–æ —Ü–µ "
            "—ñ –¥–∞–π –∑–∞–≥–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–Ω–∞–Ω—å –ø—Ä–æ —Ç—Ä—É–¥–æ–≤–µ –ø—Ä–∞–≤–æ –£–∫—Ä–∞—ó–Ω–∏. "
            "–ü—Ä–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –ø–æ—Å–∏–ª–∞–π—Å—è –Ω–∞ —Å—Ç–∞—Ç—Ç—ñ –ö–ó–ø–ü –£–∫—Ä–∞—ó–Ω–∏. "
            "–Ø–∫—â–æ –Ω–µ –∑–Ω–∞—î—à —Ç–æ—á–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ - –∫—Ä–∞—â–µ –Ω–∞–ø—Ä–∞–≤ –¥–æ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏ –∑–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—î—é."
        )

        user_message = f"–ü–∏—Ç–∞–Ω–Ω—è: {message}"
        if context:
            user_message += f"\n{context}"
        else:
            user_message += "\n\n–ü—Ä–∏–º—ñ—Ç–∫–∞: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –±–∞–∑—ñ –∑–Ω–∞–Ω—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ."

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"‚ùå GPT error: {e}")
        return (
            "üîç –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å.\n"
            "üìç –ó–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ –ø—Ä–æ—Ñ—Å–ø—ñ–ª–∫–∏:\n"
            "–î–Ω—ñ–ø—Ä–æ, –ø—Ä. –î.–Ø–≤–æ—Ä–Ω–∏—Ü—å–∫–æ–≥–æ, 93, –∫.327\n"
            "üìû 050 324-54-11\n"
            "üìß profpmgu@gmail.com"
        )
